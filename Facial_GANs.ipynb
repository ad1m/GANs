{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><font color='orange'>Generating Synthetic Faces Using Generative Adversarial Networks</font></h1></center>\n",
    "<center><h3>By: Adam Lieberman</h3></center>\n",
    "<br>\n",
    "<br>\n",
    "<h4>Introduction:</h4>\n",
    "<p>Generative Adversarial Networks (GANs) can be used to generate synthetic input data. Here, two neural networks compete against each other in a zero-sum game framework. The generator generates synthetic input from a particular distribution of interest while the discriminator simultaneously tries to discriminate between the synthesized instances and the isntances from the true data distribution. Each network controls each others loss function, i.e. the generator's training objective is to increase the error rate of the discriminator network. This means that the generator is trying to trick the discriminator into producing new synthetic instances that appear to have come from the true data distribution. In this lesson, we will use a generative adversarial network to construct synthetic facial images.</p>\n",
    "<br>\n",
    "<h4>Library Imports:</h4>\n",
    "<p>We start by importing the following libraries:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data:</h4>\n",
    "<p>We will be using the Labeled Faces in the Wild (LFW) database. This consists of more than 13,000 images of faces collected from the web. Additionally, each face comes with a label representing the name of the person pictured. To learn more about the dataset please refer to this <a href=\"http://vis-www.cs.umass.edu/lfw/\">link</a>. Here you can learn more about the data and download it to your machine. This data is also available through sklearn using the fetch_lfw_people function. Here we can specify a slice to extract a facial image and resize the each of the images to our desired shape. We load the data as follows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape:  (13233, 784)\n",
      "number of images: 13233\n",
      "number of pixels per image: 784\n"
     ]
    }
   ],
   "source": [
    "lfw_people = fetch_lfw_people(slice_=(slice(70, 195, None), slice(70, 195, None)), resize=0.224) \n",
    "data = lfw_people.data \n",
    "\n",
    "print(\"Data Shape: \",data.shape)\n",
    "num_images, num_pixels = data.shape\n",
    "print(\"number of images: %d\" % num_images)\n",
    "print(\"number of pixels per image: %d\" % num_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We see that we have 13,233 facial images that are shape 28 x 28, which is flattened into a one dimensional vector of length 784. Additionally, we note that these images are grayscale. We can visualize a few of the original images from our dataset below:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHRFFsCOiCHZEFMGOiCLYEfE/gXo3Fx3RlcYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114b69e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5ZIEHH4iWfsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117482dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7IBIjR0QqbED4v/EskTHs8mMTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1174870f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(1,1))\n",
    "im = Image.fromarray(data[4].reshape(28,28))\n",
    "p = plt.imshow(im)\n",
    "plt.show(p)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(1,1))\n",
    "p1 = plt.imshow(Image.fromarray(data[45].reshape(28,28)))\n",
    "plt.show(p1)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(1,1))\n",
    "p2 = plt.imshow(Image.fromarray(data[599].reshape(28,28)))\n",
    "plt.show(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Deep Convolutional Generative Adversarial Networks:</h4>\n",
    "<p>In 2016 Alec Radford, Luke Metz, and Soumith Chintala published <a href=\"https://arxiv.org/pdf/1511.06434.pdf\">Unsupervised Representation Learning With Deep Convolutional\n",
    "Generative Adversarial Networks</a>. In computer vision tasks, convolutional networks and supervised learning has had great success and popularity. However, unsupervised learning with convolutional neural networks has recieved less attention. Deep Convolutional Generative Adversarial Networks (DCGANs) demonstrate that they are a strong candidate for unsupervised learning. The generator/discriminator pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Since we are working with facial image data, implementing a DCGAN is a great choice. The architecture from the paper is as follows:</p>\n",
    "<ul>\n",
    "<li>Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator)</li>\n",
    "<li>Use batchnorm in both the generator and the discriminator</li>\n",
    "<li>Remove fully connected hidden layers for deeper architectures</li>\n",
    "<li>Use ReLU activation in generator for all layers except for the output, which uses Tanh</li>\n",
    "<li>Use LeakyReLU activation in the discriminator for all layers</li>\n",
    "</ul>\n",
    "<p>To build our DCGAN, we will use TensorFlow-Slim. This will allow us to build a complex network while keeping the model's architecure transparent.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>LeakyReLU layer:</h4>\n",
    "<p>When we use GANs we want to avoid sparse gradients to prevent suffering in terms of GAN stability. This means we should stear clear from ReLU and MaxPool. Instead we can use a LeakyReLU layer. Let us create one that we will call in our discriminator. We build it as follows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a LeakyReLU, good in both G and D\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Generator:</h4>\n",
    "<p>Let us now use tensorflow and create our generator network. Our architecture is as follows:</p>\n",
    "<ul>\n",
    "<li>Fully Connected Layer</li>\n",
    "<li>Reshape</li>\n",
    "<li>Convolutional Layer 1 - batch normalization, relu activation</li>\n",
    "<li>Convolutional Layer 2 - batch normalization, relu activation</li>\n",
    "<li>Convolutional Layer 3 - batch normalization, relu activation</li>\n",
    "<li>Convolutional Layer 4 - batch normalization, tanh activation</li>\n",
    "</ul>\n",
    "<p>We construct these layers in our generator as follows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    zP = slim.fully_connected(z,4*4*256,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    \n",
    "    zCon = tf.reshape(zP,[-1,4,4,256])\n",
    "    \n",
    "    gen1 = slim.convolution2d_transpose(\\\n",
    "        zCon,num_outputs=64,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    \n",
    "    gen2 = slim.convolution2d_transpose(\\\n",
    "        gen1,num_outputs=32,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    \n",
    "    gen3 = slim.convolution2d_transpose(\\\n",
    "        gen2,num_outputs=16,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    \n",
    "    g_out = slim.convolution2d_transpose(\\\n",
    "        gen3,num_outputs=1,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We have now created our generator.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Discriminator:</h4>\n",
    "<p>Our discriminator will have the following architecture:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(bottom, reuse=False):\n",
    "    dis1 = slim.convolution2d(bottom,16,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
    "    \n",
    "    dis2 = slim.convolution2d(dis1,32,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
    "    \n",
    "    dis3 = slim.convolution2d(dis2,64,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
    "    \n",
    "    d_out = slim.fully_connected(slim.flatten(dis3),1,activation_fn=tf.nn.sigmoid,\\\n",
    "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
    "    \n",
    "    return d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "z_size = 100 #Size of z vector used for generator.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These two placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32) #Real images\n",
    "\n",
    "Gz = generator(z_in) #Generates images from random z vectors\n",
    "Dx = discriminator(real_in) #Produces probabilities for real images\n",
    "Dg = discriminator(Gz,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(tf.log(Dg)) #This optimizes the generator.\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss,tvars[0:9]) #Only update the weights for the generator network.\n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Save Generated Images:</h4>\n",
    "<p>Let us now write a function to save our generated images. Here we will take in a vector of images, a size, and a path to save them. This will return us a block of images (6 x 6) so we can see how our model improves over time.We do so as follows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_generated_images(images,size,image_path): \n",
    "    images = (images+1.)/2.\n",
    "    height = images.shape[1]\n",
    "    width = images.shape[2]\n",
    "    img = np.zeros((height * size[0], width * size[1]))\n",
    "    for idx, image in enumerate(images):\n",
    "        a = idx % size[1]\n",
    "        b = idx // size[1]\n",
    "        img[b*height:b*height+height, a*width:a*width+width] = image\n",
    "    sve = scipy.misc.imsave(image_path,img)\n",
    "    return sve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  0 / 5000\n",
      "Gen Loss: 0.364692 Disc Loss: 1.58653\n",
      "Progress:  1 / 5000\n",
      "Progress:  2 / 5000\n",
      "Progress:  3 / 5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6117fec21b27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the generator, twice for good measure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Adamlieberman/anaconda/envs/cse6240hw1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Adamlieberman/anaconda/envs/cse6240hw1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Adamlieberman/anaconda/envs/cse6240hw1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Adamlieberman/anaconda/envs/cse6240hw1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Adamlieberman/anaconda/envs/cse6240hw1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128 #Size of image batch to apply at each iteration.\n",
    "iterations = 5000 #Total number of iterations to use.\n",
    "sample_directory = './album_art_figs' #Directory to save sample images from generator in.\n",
    "model_directory = './album_art_models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        print(\"Progress: \",i,\"/\",iterations)\n",
    "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        r = random.sample(range(data.shape[0]), batch_size)\n",
    "        xs = data[r]\n",
    "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1))\n",
    "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs}) #Update the discriminator\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs}) #Update the generator, twice for good measure.\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs})\n",
    "        if i % 10 == 0:\n",
    "            print(\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss))\n",
    "            z2 = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate another z batch\n",
    "            newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            save_generated_images(np.reshape(newZ[0:36],[36,32,32]),[6,6],sample_directory+'/fig'+str(i)+'.png')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
